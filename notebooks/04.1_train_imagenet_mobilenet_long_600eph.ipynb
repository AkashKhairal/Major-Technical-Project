{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "327a8a7c",
   "metadata": {},
   "source": [
    "# EAST Scene Text Detection (Scratch Training) â€“ ICDAR2015\n",
    "\n",
    "**Objective**  \n",
    "To build an end-to-end scene text detection pipeline using EAST, train it from scratch,\n",
    "and establish a baseline for further lightweight detector research.\n",
    "\n",
    "**Key Focus**\n",
    "- End-to-end pipeline correctness\n",
    "- Proper evaluation using ICDAR2015 protocol\n",
    "- Recording accuracy + efficiency metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f595380a",
   "metadata": {},
   "source": [
    "## Project setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "PROJECT_ROOT = \"/DATA/akash/akash_cnn/lightweight-text-detector\"\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a78b78",
   "metadata": {},
   "source": [
    "## Experiment CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# EXPERIMENT CONTROL\n",
    "# ===============================\n",
    "\n",
    "EXPERIMENT_NAME = \"exp4.1_imagenet_mobilenet_long_600eph\"\n",
    "\n",
    "BACKBONE = \"mobilenetv2\"      # ðŸ”¥ NEW\n",
    "USE_PRETRAINED = True\n",
    "PRETRAINED_TYPE = \"imagenet\"\n",
    "\n",
    "INPUT_SIZE = 512\n",
    "EPOCHS = 600\n",
    "BATCH_SIZE = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b3ca2",
   "metadata": {},
   "source": [
    "## Experiment Folder Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ecb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "EXP_ROOT = f\"experiments/{EXPERIMENT_NAME}\"\n",
    "\n",
    "WEIGHTS_DIR = f\"{EXP_ROOT}/weights\"\n",
    "LOG_DIR = f\"{EXP_ROOT}/logs\"\n",
    "RESULTS_DIR = f\"{EXP_ROOT}/results\"\n",
    "PRED_DIR = f\"{RESULTS_DIR}/predictions\"\n",
    "\n",
    "for d in [WEIGHTS_DIR, LOG_DIR, PRED_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"Experiment directories ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b71a99a",
   "metadata": {},
   "source": [
    "## Dataset Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2433503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMG_DIR = \"data/icdar2015/ch4_train_images\"\n",
    "TEST_IMG_DIR  = \"data/icdar2015/ch4_test_images\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5f59d0",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e3ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.east import EAST\n",
    "import torch\n",
    "\n",
    "model = EAST(\n",
    "    cfg=\"D\",\n",
    "    weights=\"imagenet\" if USE_PRETRAINED else None,\n",
    "    backbone=BACKBONE\n",
    ")\n",
    "\n",
    "model = torch.nn.DataParallel(model)   # ðŸ”¥ MAIN FIX\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "print(\"EAST initialized with DataParallel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5fe989",
   "metadata": {},
   "source": [
    "## Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fd257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.losses.loss import Loss\n",
    "\n",
    "criterion = Loss()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af8418",
   "metadata": {},
   "source": [
    "## DATASET + DATALOADER CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464aba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from src.data.dataset import Dataset\n",
    "\n",
    "# -------------------------\n",
    "# Dataset\n",
    "# -------------------------\n",
    "train_dataset = Dataset(\n",
    "    data_path=TRAIN_IMG_DIR,\n",
    "    scale=0.25,\n",
    "    length=INPUT_SIZE\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# DataLoader\n",
    "# -------------------------\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(\"Train loader ready\")\n",
    "print(\"Total training samples:\", len(train_dataset))\n",
    "print(\"Batches per epoch:\", len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc271632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "print(\"AMP GradScaler initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa4b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "print(\"Starting fresh training from epoch 0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beda477",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16661553",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json   # ðŸ”¥ ADD (if not already imported)\n",
    "\n",
    "model.train()\n",
    "loss_log = []\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "\n",
    "    epoch_geo_loss = 0.0\n",
    "    epoch_cls_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        train_loader,\n",
    "        desc=f\"Epoch [{epoch+1}/{EPOCHS}]\",\n",
    "        dynamic_ncols=True\n",
    "    )\n",
    "\n",
    "    for imgs, gt_score, gt_geo, ignored_map in pbar:\n",
    "\n",
    "        # -------------------------\n",
    "        # Move to device\n",
    "        # -------------------------\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "        gt_score = gt_score.to(DEVICE, non_blocking=True)\n",
    "        gt_geo = gt_geo.to(DEVICE, non_blocking=True)\n",
    "        ignored_map = ignored_map.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # -------------------------\n",
    "        # Forward + EAST loss (AMP)\n",
    "        # -------------------------\n",
    "        with autocast():\n",
    "            pred_score, pred_geo = model(imgs)\n",
    "\n",
    "            # ðŸ”¥ geometry loss FP32\n",
    "            loss_dict = criterion(\n",
    "                gt_score.float(), pred_score.float(),\n",
    "                gt_geo.float(), pred_geo.float(),\n",
    "                ignored_map.float()\n",
    "            )\n",
    "\n",
    "            geo_loss = loss_dict[\"geo_loss\"]\n",
    "            cls_loss = loss_dict[\"cls_loss\"]\n",
    "            total_loss = geo_loss + cls_loss\n",
    "\n",
    "        # -------------------------\n",
    "        # Backward\n",
    "        # -------------------------\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # -------------------------\n",
    "        # Logging\n",
    "        # -------------------------\n",
    "        epoch_geo_loss += geo_loss.item()\n",
    "        epoch_cls_loss += cls_loss.item()\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            geo=f\"{geo_loss.item():.3f}\",\n",
    "            cls=f\"{cls_loss.item():.3f}\"\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # Epoch summary\n",
    "    # -------------------------\n",
    "    avg_geo = epoch_geo_loss / len(train_loader)\n",
    "    avg_cls = epoch_cls_loss / len(train_loader)\n",
    "\n",
    "    loss_log.append({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"geo_loss\": avg_geo,\n",
    "        \"cls_loss\": avg_cls\n",
    "    })\n",
    "\n",
    "    # ðŸ”¥ NEW: SAVE LOSS LOG EVERY EPOCH (CRASH-SAFE)\n",
    "    with open(f\"{LOG_DIR}/loss_log.json\", \"w\") as f:\n",
    "        json.dump(loss_log, f, indent=2)\n",
    "\n",
    "    print(\n",
    "        f\"\\nEpoch {epoch+1} Summary | \"\n",
    "        f\"Geo Loss: {avg_geo:.4f} | \"\n",
    "        f\"Cls Loss: {avg_cls:.4f}\"\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Save checkpoint (FULL STATE)\n",
    "    # -------------------------\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scaler\": scaler.state_dict(),\n",
    "            },\n",
    "            f\"{WEIGHTS_DIR}/epoch_{epoch+1}.pth\"\n",
    "        )\n",
    "\n",
    "print(\"\\nTraining finished successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c224cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c2720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8f0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46c11e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee8343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c1540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchEAST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
